import numpy as np
import pandas as pd
import re
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE
import pyswarm
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class SentimentPipeline:
    def __init__(self, model_name='vinai/bertweet-base', device='cuda' if torch.cuda.is_available() else 'cpu'):
        """
        Initialize the sentiment analysis pipeline
        
        Args:
            model_name: BERTweet model identifier
            device: 'cuda' or 'cpu'
        """
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(device)
        self.model.eval()
        self.scaler = StandardScaler()
        self.svm_model = None
        self.best_params = None
        
    def preprocess_text(self, text):
        """
        Preprocess social media text
        
        Args:
            text: Raw social media text
            
        Returns:
            Cleaned text
        """
        # Convert to string and lowercase
        text = str(text).lower()
        
        # Replace URLs with URL token
        text = re.sub(r'http\S+|www.\S+', 'httpurl', text)
        
        # Replace mentions with @USER token (BERTweet specific)
        text = re.sub(r'@\w+', '@user', text)
        
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # Keep hashtags but remove the # symbol
        text = re.sub(r'#(\w+)', r'\1', text)
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Remove very short text
        if len(text.split()) < 3:
            return None
            
        return text
    
    def extract_embeddings(self, texts, batch_size=16):
        """
        Extract BERTweet embeddings for texts
        
        Args:
            texts: List of preprocessed texts
            batch_size: Batch size for processing
            
        Returns:
            numpy array of embeddings
        """
        embeddings = []
        
        # Process in batches
        for i in tqdm(range(0, len(texts), batch_size), desc="Extracting embeddings"):
            batch_texts = texts[i:i+batch_size]
            
            # Tokenize
            encoded = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors='pt'
            ).to(self.device)
            
            # Get embeddings
            with torch.no_grad():
                outputs = self.model(**encoded)
                # Use CLS token embeddings
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_embeddings)
        
        return np.vstack(embeddings)
    
    def add_additional_features(self, texts):
        """
        Add additional features beyond embeddings
        
        Args:
            texts: List of preprocessed texts
            
        Returns:
            numpy array of additional features
        """
        features = []
        
        for text in texts:
            feat = []
            # Length features
            feat.append(len(text.split()))
            feat.append(len(text))
            
            # Punctuation features
            feat.append(text.count('!'))
            feat.append(text.count('?'))
            feat.append(text.count('...'))
            
            # Capitalization (check original before lowercasing)
            caps_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)
            feat.append(caps_ratio)
            
            # Emoji indicators (simplified)
            emoji_patterns = [':)', ':(', ':D', '😀', '😢', '😡', '👍', '👎']
            feat.append(sum(1 for emoji in emoji_patterns if emoji in text))
            
            features.append(feat)
            
        return np.array(features)
    
    def pso_svm_optimization(self, X_train, y_train, n_particles=20, max_iter=50):
        """
        Optimize SVM hyperparameters using Particle Swarm Optimization
        
        Args:
            X_train: Training features
            y_train: Training labels
            n_particles: Number of particles in swarm
            max_iter: Maximum iterations
            
        Returns:
            Best parameters found
        """
        # Define objective function
        def objective(params):
            C, gamma = params
            
            # Create SVM with current parameters
            svm = SVC(C=C, gamma=gamma, kernel='rbf', random_state=42)
            
            # Use stratified k-fold cross-validation
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            scores = cross_val_score(svm, X_train, y_train, cv=skf, scoring='f1_weighted')
            
            # Return negative score (PSO minimizes)
            return -scores.mean()
        
        # Define bounds for C and gamma
        lb = [0.1, 0.001]  # Lower bounds
        ub = [100, 1]      # Upper bounds
        
        print("Starting PSO optimization...")
        # Run PSO
        best_params, _ = pyswarm.pso(
            objective, 
            lb, 
            ub,
            swarmsize=n_particles,
            maxiter=max_iter,
            debug=True
        )
        
        return {'C': best_params[0], 'gamma': best_params[1]}
    
    def train_with_cv(self, X, y, use_pso=True, n_folds=5, test_size=0.2):
        """
        Train the pipeline with k-fold cross-validation
        
        Args:
            X: Feature matrix (embeddings + additional features)
            y: Labels (0 for negative, 1 for positive)
            use_pso: Whether to use PSO for hyperparameter optimization
            n_folds: Number of folds for cross-validation
            test_size: Proportion of data for final testing
            
        Returns:
            Dictionary with evaluation metrics
        """
        # First split out a test set that won't be touched during CV
        X_train_val, X_test, y_train_val, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Initialize metrics storage
        cv_metrics = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'confusion_matrices': []
        }
        
        # Perform k-fold cross-validation
        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
        
        print(f"\nPerforming {n_folds}-fold cross-validation...")
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val), 1):
            print(f"\nFold {fold}/{n_folds}")
            print("-" * 30)
            
            # Split data for this fold
            X_fold_train = X_train_val[train_idx]
            X_fold_val = X_train_val[val_idx]
            y_fold_train = y_train_val[train_idx]
            y_fold_val = y_train_val[val_idx]
            
            # Scale features
            scaler_fold = StandardScaler()
            X_fold_train_scaled = scaler_fold.fit_transform(X_fold_train)
            X_fold_val_scaled = scaler_fold.transform(X_fold_val)
            
            # Apply SMOTE to training fold
            smote = SMOTE(random_state=42)
            X_fold_balanced, y_fold_balanced = smote.fit_resample(X_fold_train_scaled, y_fold_train)
            print(f"SMOTE applied: {len(y_fold_train)} -> {len(y_fold_balanced)} samples")
            
            # Optimize hyperparameters (only on first fold if using PSO to save time)
            if fold == 1 and use_pso:
                self.best_params = self.pso_svm_optimization(X_fold_balanced, y_fold_balanced)
                print(f"Best params: C={self.best_params['C']:.3f}, gamma={self.best_params['gamma']:.6f}")
            elif not use_pso:
                self.best_params = {'C': 1.0, 'gamma': 'scale'}
            
            # Train SVM for this fold
            svm_fold = SVC(
                C=self.best_params['C'],
                gamma=self.best_params['gamma'],
                kernel='rbf',
                random_state=42,
                probability=True
            )
            svm_fold.fit(X_fold_balanced, y_fold_balanced)
            
            # Evaluate on validation fold
            y_fold_pred = svm_fold.predict(X_fold_val_scaled)
            
            # Store metrics
            cv_metrics['accuracy'].append(accuracy_score(y_fold_val, y_fold_pred))
            cv_metrics['precision'].append(precision_score(y_fold_val, y_fold_pred, average='weighted'))
            cv_metrics['recall'].append(recall_score(y_fold_val, y_fold_pred, average='weighted'))
            cv_metrics['f1'].append(f1_score(y_fold_val, y_fold_pred, average='weighted'))
            cv_metrics['confusion_matrices'].append(confusion_matrix(y_fold_val, y_fold_pred))
            
            print(f"Fold {fold} F1-Score: {cv_metrics['f1'][-1]:.3f}")
        
        # Calculate CV statistics
        cv_results = {
            'cv_accuracy_mean': np.mean(cv_metrics['accuracy']),
            'cv_accuracy_std': np.std(cv_metrics['accuracy']),
            'cv_precision_mean': np.mean(cv_metrics['precision']),
            'cv_precision_std': np.std(cv_metrics['precision']),
            'cv_recall_mean': np.mean(cv_metrics['recall']),
            'cv_recall_std': np.std(cv_metrics['recall']),
            'cv_f1_mean': np.mean(cv_metrics['f1']),
            'cv_f1_std': np.std(cv_metrics['f1']),
            'cv_all_folds': cv_metrics
        }
        
        # Train final model on entire train_val set with best parameters
        print("\nTraining final model on full training set...")
        
        # Scale full training set
        self.scaler.fit(X_train_val)
        X_train_val_scaled = self.scaler.transform(X_train_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Apply SMOTE to full training set
        smote_final = SMOTE(random_state=42)
        X_train_balanced_final, y_train_balanced_final = smote_final.fit_resample(
            X_train_val_scaled, y_train_val
        )
        
        # Train final model
        self.svm_model = SVC(
            C=self.best_params['C'],
            gamma=self.best_params['gamma'],
            kernel='rbf',
            random_state=42,
            probability=True
        )
        self.svm_model.fit(X_train_balanced_final, y_train_balanced_final)
        
        # Final evaluation on held-out test set
        y_test_pred = self.svm_model.predict(X_test_scaled)
        
        final_metrics = {
            'test_accuracy': accuracy_score(y_test, y_test_pred),
            'test_precision': precision_score(y_test, y_test_pred, average='weighted'),
            'test_recall': recall_score(y_test, y_test_pred, average='weighted'),
            'test_f1': f1_score(y_test, y_test_pred, average='weighted'),
            'test_confusion_matrix': confusion_matrix(y_test, y_test_pred),
            **cv_results
        }
        
        return final_metrics
    
    def train(self, X, y, use_pso=True, test_size=0.3):
        """
        Train the complete pipeline (backwards compatibility method)
        
        Args:
            X: Feature matrix (embeddings + additional features)
            y: Labels (0 for negative, 1 for positive)
            use_pso: Whether to use PSO for hyperparameter optimization
            test_size: Proportion of data for testing
            
        Returns:
            Dictionary with evaluation metrics
        """
        # Call the new CV method with n_folds=1 for backwards compatibility
        return self.train_with_cv(X, y, use_pso=use_pso, n_folds=5, test_size=test_size)
    
    def predict(self, texts):
        """
        Predict sentiment for new texts
        
        Args:
            texts: List of texts to classify
            
        Returns:
            Predictions and probabilities
        """
        # Preprocess texts
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [t for t in processed_texts if t is not None]
        
        if not processed_texts:
            return None, None
        
        # Extract features
        embeddings = self.extract_embeddings(processed_texts)
        additional_features = self.add_additional_features(processed_texts)
        X = np.hstack([embeddings, additional_features])
        
        # Scale features
        X_scaled = self.scaler.transform(X)
        
        # Predict
        predictions = self.svm_model.predict(X_scaled)
        probabilities = self.svm_model.predict_proba(X_scaled)
        
        return predictions, probabilities

# Main execution function
def run_pipeline(data_path=None):
    """
    Run the complete sentiment analysis pipeline
    
    Args:
        data_path: Path to CSV file with 'text' and 'sentiment' columns
    """
    # Sample data creation (replace with your actual data loading)
    if data_path is None:
        print("Creating sample data...")
        sample_data = pd.DataFrame({
            'text': [
                "This company is doing amazing! Stock going to the moon 🚀",
                "Terrible earnings report, selling everything",
                "Great product launch today! Very impressed",
                "Management is incompetent, avoid at all costs",
                "Neutral observation about the company performance",
                "Love their new strategy, buying more shares!",
                "Worst customer service ever encountered",
                "Solid fundamentals, good long-term hold",
                "Bankruptcy incoming, get out now!",
                "Innovative technology, bright future ahead"
            ] * 50,  # Replicate for more samples
            'sentiment': [1, 0, 1, 0, 1, 1, 0, 1, 0, 1] * 50
        })
    else:
        sample_data = pd.read_csv(data_path)
    
    # Initialize pipeline
    print("Initializing pipeline...")
    pipeline = SentimentPipeline()
    
    # Preprocess texts
    print("Preprocessing texts...")
    processed_texts = []
    valid_labels = []
    
    for idx, row in sample_data.iterrows():
        processed = pipeline.preprocess_text(row['text'])
        if processed:
            processed_texts.append(processed)
            valid_labels.append(row['sentiment'])
    
    # Extract embeddings
    print("Extracting BERTweet embeddings...")
    embeddings = pipeline.extract_embeddings(processed_texts)
    
    # Add additional features
    print("Adding additional features...")
    additional_features = pipeline.add_additional_features(processed_texts)
    
    # Combine features
    X = np.hstack([embeddings, additional_features])
    y = np.array(valid_labels)
    
    print(f"Feature matrix shape: {X.shape}")
    print(f"Class distribution: {np.bincount(y)}")
    
    # Train model with cross-validation
    print("\nTraining model with 5-fold cross-validation and PSO optimization...")
    metrics = pipeline.train_with_cv(X, y, use_pso=True, n_folds=5)
    
    # Print results
    print("\n" + "="*50)
    print("CROSS-VALIDATION RESULTS")
    print("="*50)
    print(f"CV Accuracy:  {metrics['cv_accuracy_mean']:.3f} (+/- {metrics['cv_accuracy_std']:.3f})")
    print(f"CV Precision: {metrics['cv_precision_mean']:.3f} (+/- {metrics['cv_precision_std']:.3f})")
    print(f"CV Recall:    {metrics['cv_recall_mean']:.3f} (+/- {metrics['cv_recall_std']:.3f})")
    print(f"CV F1-Score:  {metrics['cv_f1_mean']:.3f} (+/- {metrics['cv_f1_std']:.3f})")
    
    print("\n" + "="*50)
    print("FINAL TEST SET RESULTS")
    print("="*50)
    print(f"Test Accuracy:  {metrics['test_accuracy']:.3f}")
    print(f"Test Precision: {metrics['test_precision']:.3f}")
    print(f"Test Recall:    {metrics['test_recall']:.3f}")
    print(f"Test F1-Score:  {metrics['test_f1']:.3f}")
    print(f"\nTest Confusion Matrix:")
    print(metrics['test_confusion_matrix'])
    
    # Test prediction on new samples
    print("\n" + "="*50)
    print("TESTING ON NEW SAMPLES")
    print("="*50)
    test_texts = [
        "This company is going bankrupt for sure!",
        "Excellent quarterly results, exceeding expectations",
        "Not sure about their future prospects"
    ]
    
    predictions, probabilities = pipeline.predict(test_texts)
    
    for text, pred, prob in zip(test_texts, predictions, probabilities):
        sentiment = "Positive" if pred == 1 else "Negative"
        confidence = max(prob) * 100
        print(f"\nText: {text}")
        print(f"Sentiment: {sentiment} (Confidence: {confidence:.1f}%)")
    
    return pipeline

# Run the pipeline
if __name__ == "__main__":
    # To use your own data, pass the CSV file path
    # pipeline = run_pipeline('your_data.csv')
    pipeline = run_pipeline()
